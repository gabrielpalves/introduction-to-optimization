{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "from plot_descent_penalty import plot_2d_contour, plot_3d_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHAR OUTRO PROBLEMA\n",
    "# MODIFICAR ALGORITMOS DOS PLOTS\n",
    "# ORGANIZAR MELHOR O CÃ“DIGO\n",
    "# SQP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems to optimize (define a new problem here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problem(problem):\n",
    "    \"\"\"Get the problem to optimize.\n",
    "    Problem 1: Example 5.8 (Joaquim R. R. A. Martins, Andrew Ning - Engineering Design Optimization (2021))\n",
    "    Problem 2: Something from nothing\n",
    "\n",
    "    Args:\n",
    "        problem (int): problem selected\n",
    "\n",
    "    Returns:\n",
    "        obj_fun (dict): contains the objective function (key = 'fun')\n",
    "                        and its gradient (key = 'jac')\n",
    "        eq_cons (dict): contains the equality constraints (key = 'fun')\n",
    "                        and its gradients (key = 'jac')\n",
    "        ineq_cons (dict): contains the inequality constraints (key = 'fun')\n",
    "                          and its gradients (key = 'jac')\n",
    "    \"\"\"\n",
    "    \n",
    "    match problem:\n",
    "        # Problem 1 - Example 5.8 (Joaquim R. R. A. Martins, Andrew Ning - Engineering Design Optimization (2021))\n",
    "        case 1:\n",
    "            # Objective function\n",
    "            obj_fun = {\n",
    "                'type': 'obj',\n",
    "                'fun' : lambda x : np.array([x[0] + 2*x[1]]),\n",
    "                'jac' : lambda x : np.array([1, 2])\n",
    "            }\n",
    "            \n",
    "            # Equality constraints\n",
    "            eq_cons = {\n",
    "                'type': 'eq',\n",
    "                'fun' : lambda x: np.array([]),\n",
    "                'jac' : lambda x: np.array([])\n",
    "                }\n",
    "            \n",
    "            # Inequality constraints\n",
    "            ineq_cons = {\n",
    "                'type': 'ineq',\n",
    "                'fun' : lambda x: np.array([1/4*x[0]**2 + x[1]**2 - 1]),\n",
    "                'jac' : lambda x: np.array([1/2*x[0], 2*x[1]])\n",
    "                }\n",
    "        \n",
    "        # Problem 2 - Something from nothing\n",
    "        case 2:\n",
    "            # Objective function\n",
    "            obj_fun = {\n",
    "                'type': 'obj',\n",
    "                'fun' : lambda x : np.array([x[0] + 2*x[1]]),\n",
    "                'jac' : lambda x : np.array([1, 2])\n",
    "            }\n",
    "            \n",
    "            # Equality constraints\n",
    "            eq_cons = {\n",
    "                'type': 'eq',\n",
    "                'fun' : lambda x: np.array([]),\n",
    "                'jac' : lambda x: np.array([])\n",
    "                }\n",
    "            \n",
    "            # Inequality constraints\n",
    "            ineq_cons = {\n",
    "                'type': 'ineq',\n",
    "                'fun' : lambda x: np.array([1/4*x[0]**2 + x[1]**2 - 1]),\n",
    "                'jac' : lambda x: np.array([1/2*x[0], 2*x[1]])\n",
    "                }\n",
    "    \n",
    "    return obj_fun, eq_cons, ineq_cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exterior penalty methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method(method):\n",
    "    \"\"\"Definition of the penalized Lagrangian (transformed functional phi)\n",
    "\n",
    "    Args:\n",
    "        method (int): selected function phi\n",
    "\n",
    "    Returns:\n",
    "        phi (function): a method that receives a point 'x' (numpy array of shape (2,))\n",
    "                        and returns the value of phi and its gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    match method:\n",
    "        # Quadratic penalty\n",
    "        case 1:\n",
    "            def phi(x):\n",
    "                f, df, h, dh, g, dg = get_values(x)\n",
    "                \n",
    "                auxg = np.maximum(0, g)\n",
    "                ph = f + mu*(h.sum()**2 + auxg.sum()**2)\n",
    "                \n",
    "                # Construction of the gradient of phi: contribution of the inequality constraints (g_i)\n",
    "                dgaux = np.array(np.zeros(x.shape))\n",
    "                if g.size == 1:  # split into two situations: with only one constraints, and more constraints\n",
    "                    dgaux = dg*np.maximum(0, g)\n",
    "                else:\n",
    "                    for i in range(g.size):\n",
    "                        dgaux = dgaux + dg[i, :]*np.maximum(0, g[i])\n",
    "                    \n",
    "                # Construction of the gradient of phi: contribution of the equality constraints (h_j)\n",
    "                dhaux = np.array(np.zeros(x.shape))\n",
    "                if h.size == 1:  # split into two situations: with only one constraints, and more constraints\n",
    "                    dhaux = dh*h\n",
    "                else:\n",
    "                    for j in range(h.size):\n",
    "                        dhaux = dhaux + dh[j, :]*h[j]\n",
    "                \n",
    "                # Gradient of phi:\n",
    "                dph = df + 2*mu*(dhaux + dgaux)\n",
    "                \n",
    "                return ph, dph\n",
    "        \n",
    "        # Augmented Lagrangian\n",
    "        case 2:\n",
    "            def phi(x):\n",
    "                f, df, h, dh, g, dg = get_values(x)\n",
    "                \n",
    "                ghat = np.copy(g)\n",
    "                for i in range(g.size):\n",
    "                    c = -lambda_ineq[i]/mu\n",
    "                    if g[i] < c:\n",
    "                        ghat[i] = c\n",
    "\n",
    "                # maxg = np.max(0, ghat)\n",
    "                L = f + np.dot(lambda_eq, h) + np.dot(lambda_ineq, ghat) + mu/2 * (np.dot(h, h) + np.dot(ghat, ghat))\n",
    "                \n",
    "                # transform in matrix\n",
    "                if dh.size > 0 and len(dh.shape) == 1:\n",
    "                    dh = np.array([dh])\n",
    "                    \n",
    "                if dg.size > 0 and len(dg.shape) == 1:\n",
    "                    dg = np.array([dg])\n",
    "                    \n",
    "                dL = df + np.matmul(lambda_eq, dh) + np.matmul(lambda_ineq, dg) + mu*(np.matmul(h, dh) + np.matmul(g, dg))\n",
    "                \n",
    "                return L, dL\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(x):\n",
    "    \"\"\"Get the values of the objective function,\n",
    "    equality constraints, inequality constraints\n",
    "    and all its gradients\n",
    "\n",
    "    Args:\n",
    "        x (numpy array): point [x1, x2]\n",
    "\n",
    "    Returns:\n",
    "        values of gradients\n",
    "    \"\"\"\n",
    "    obj_fun, eq_cons, ineq_cons = get_problem(problem)\n",
    "    \n",
    "    f, df = obj_fun['fun'](x), obj_fun['jac'](x)\n",
    "    h, dh = eq_cons['fun'](x), eq_cons['jac'](x)\n",
    "    g, dg = ineq_cons['fun'](x), ineq_cons['jac'](x)\n",
    "    \n",
    "    return f, df, h, dh, g, dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters (user has to define)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = Example 5.8 (Joaquim R. R. A. Martins, Andrew Ning - Engineering Design Optimization (2021))\n",
    "# 2 = Something from nothing\n",
    "problem = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 5*np.array([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper bound for the Golden Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial value for penalization (mu) and rate of increase (rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.1\n",
    "rho = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence tolerance for the minimization of Lagrangian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TolG = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping criterias for the exterior penalty method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itmax = 10 # Maximum number of iterations\n",
    "epsilon1 = 0.001 # Magnitude of the penalty terms\n",
    "epsilon2 = 0.001 # Change in value of the penalized objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the method of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = Quadratic penalty\n",
    "# 2 = Augmented Lagrangian\n",
    "method = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the problem and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function, equality_constraints, inequality_constraints = get_problem(problem)\n",
    "phi = get_method(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient + Golden Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_alpha(alpha, args):\n",
    "    \"\"\"Definition of the equation to be minimized as function of the step size alpha\n",
    "\n",
    "    Args:\n",
    "        alpha (float): step size\n",
    "        args (array): array with arguments point (x) and direction of search (d)\n",
    "\n",
    "    Returns:\n",
    "        f (float): value of phi\n",
    "    \"\"\"\n",
    "    x = args[0] + alpha*args[1]\n",
    "    f, _ = phi(x)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG_GS(x, alpha0, TolG):\n",
    "    # Count variable\n",
    "    t = 0\n",
    "    # f and df values at the initial point\n",
    "    [f, df] = phi(x)\n",
    "    dftm1 = df\n",
    "    \n",
    "    xs = [x]\n",
    "    fs = [f]\n",
    "        \n",
    "    while np.sqrt(df @ df) > TolG:\n",
    "        # Search direction: Conjugated Gradient\n",
    "        beta = (np.linalg.norm(df)/np.linalg.norm(dftm1))**2\n",
    " \n",
    "        if t == 0:\n",
    "            d = -df\n",
    "        else:\n",
    "            d = -df + beta*dtm1\n",
    "            \n",
    "        # Step determination: Golden Search (method='golden'), Brent (method='brent') or Bounded (method='bounded')\n",
    "        alpha = minimize_scalar(f_alpha, bounds=(.001, alpha0), args=([x, d]), method='bounded')\n",
    "\n",
    "        # Update the current point\n",
    "        xt = x + alpha.x*d\n",
    "        xs.append(xt)\n",
    "        \n",
    "        # Saves information of gradient and descent direction of current iteration\n",
    "        dftm1 = df\n",
    "        dtm1 = d\n",
    "    \n",
    "        # Evaluate the objective function and gradient at the new point\n",
    "        [f, df] = phi(xt)\n",
    "        \n",
    "        fs.append(f)\n",
    "    \n",
    "        # Update the design variable and iteration number\n",
    "        x = xt\n",
    "        t = t + 1\n",
    "    \n",
    "    return x, f, df, t, xs, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exterior penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, stop_1, stop_2 = 0, 1.0, 1.0 # for stopping criterias\n",
    "cost_f, cost_g = 0, 0 # costs\n",
    "\n",
    "_, _, h, _, g, _ = get_values(np.array([0, 0]))\n",
    "\n",
    "lambda_eq = np.zeros((h.size))\n",
    "lambda_ineq = np.zeros((g.size))\n",
    "\n",
    "points = []\n",
    "values = []\n",
    "\n",
    "while k < itmax and stop_1 > epsilon1 and stop_2 > epsilon2:\n",
    "    # Conjugate gradient + Golden search method\n",
    "    xt, f, df, t, xs, fs = CG_GS(x, alpha0, TolG) # minimize\n",
    "    \n",
    "    # Check convergence\n",
    "    fopt, _, h, dh, g, dg = get_values(xt)\n",
    "    stop_1 = abs((f - fopt)/fopt)\n",
    "    \n",
    "    if k > 0:\n",
    "        stop_2 = abs((f - f_old)/f)\n",
    "        \n",
    "    points.append(xs)\n",
    "    values.append(fs)\n",
    "    \n",
    "    f_old = f\n",
    "    \n",
    "    if k >= itmax:\n",
    "        print('Stopped due to the number of iterations')\n",
    "    elif stop_1 <= epsilon1:\n",
    "        print('Stopped due to the small magnitude of the penalty terms')\n",
    "    elif stop_2 <= epsilon2:\n",
    "        print('Stopped due to a small change in value of the penalized objective function')\n",
    "        \n",
    "    # Update Lagrange multiplier, penalty parameter and starting point\n",
    "    lambda_eq = lambda_eq + mu*h\n",
    "    lambda_ineq = lambda_ineq + mu*g\n",
    "    mu = mu*rho\n",
    "    x = xt\n",
    "    k = k + 1\n",
    "    \n",
    "    # Update cost\n",
    "    cost_f += t\n",
    "    cost_g += t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fopt, dfopt, hopt, dhopt, gopt, dgopt = get_values(x)\n",
    "\n",
    "print('Optimum found:')\n",
    "print(xt)\n",
    "print('Objective function value at the optimum:')\n",
    "print(fopt)\n",
    "print('Inequality constraints at the optimum:')\n",
    "print(gopt)\n",
    "print('Equality constraints at the optimum:')\n",
    "print(hopt)\n",
    "\n",
    "print('Number of times that the f_obj function and constraints were evaluated, respectively:')\n",
    "print(cost_f)\n",
    "print(cost_g)\n",
    "print('Number of iterations of the External penalty method:')\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = np.array(points)\n",
    "all_x = np.reshape(all_x, ( int(all_x.size/2), 2 ))\n",
    "\n",
    "all_f = np.array(values)\n",
    "all_f = np.reshape(all_f, (all_f.size))\n",
    "\n",
    "plot_2d_contour(all_x, obj_fun=objective_function, plot_h=True, plot_g=True, eq_constraints=equality_constraints, ineq_constraints=inequality_constraints)\n",
    "plot_3d_surface(all_x, all_f, obj_fun=objective_function, plot_h=True, plot_g=True, eq_constraints=equality_constraints, ineq_constraints=inequality_constraints)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
